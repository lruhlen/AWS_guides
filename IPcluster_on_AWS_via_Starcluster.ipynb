{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Amazon Web Services: Why and When It's Useful\n",
      "\n",
      "[Embarrassingly parallel](http://en.wikipedia.org/wiki/Embarrassingly_parallel) computational processes are important in astrophysics in two contexts.\n",
      "\n",
      "+ Running Monte-Carlo type simulations\n",
      "+ Data processing pipelines\n",
      "\n",
      "Computer clusters greatly speed both processes.  You may not have access to a dedicated cluster (e.g. on your campus), or the local cluster may be unsatisfactory (long queues, frequent power outages, other dependability issues). Using a cloud-based cluster gets around these issues.  Amazon Web Services (AWS) provides a stable, accessible, and standardized framework for creating your own cloud-based cluster.  \n",
      "\n",
      "AWS is a pay-as-you-go service, meaning you pay by the hour for each machine used.  While this is *not* a cost-effective way to run weeks-long processes, it is perfect for 'bursty' computational patterns.  An example of a common bursty computational pattern would be spending a few hours per week running simulations or processing data, and the rest of the week examining or debugging the results.  Bear in mind that a task that might take you all day or several days to run on your local machine can be sped up by about an order of magnitude once you're able apply more processors to it.  As such, even if it takes a solid week for your local machine to run a suite of Monte Carlo simulations, it will probably still qualify as a bursty process once you're able to execute it on an AWS cluster.\n",
      "\n",
      "## Overview of how AWS works\n",
      "\n",
      "This section provides the bare bones of what you need to know about AWS in order to rent a cluster for your scientific computations.  Google, Wikipedia, and the AWS website can furnish you with more in-depth information on how AWS works, and what its many capabilities are.  While the details of how AWS works will interest computationally adept students, this tutorial is written for students without an interest or background in computer science.\n",
      "\n",
      "Keep in mind that you are not renting real, tangible computing boxes.  Rather, AWS rents out virtual machines. All we really need to know is that Amazon has a huge room of computers, each of which can run a variety of different virtual machines. For the most part, the virtual machines we rent from Amazon behave exactly as if they were real computers.  \n",
      "\n",
      "The only differnce, albeit an important one, is that once you 'turn off' (terminate) your rented virtual machine, everything on it is *gone*: all data, results, programs, everything vanishes into thin air.  Turning off one of these rental machines is not like turning off your laptop; it's like selling your laptop, or taking a hammer to it. There are ways to hang on to your results, however, which we discuss below.\n",
      "\n",
      "To explain how the AWS system works, we'll refer to the analogous experience of renting a different kind of machine: a car.\n",
      "\n",
      "### Important terms\n",
      "\n",
      "+ **AMI**\n",
      "    - This stands for Amazon Machine Instance.  This is the 'type' of computer you choose to rent. \n",
      "        - This is like specifying what brand of car (Dodge, Toyota, etc.) you would like to rent from Avis/Hertz/etc.\n",
      "    - AMIs are available with different operating systems (Windows, OS-X/Darwin, various flavors of Linux).  \n",
      "        - They also come packaged with various software programs (python, excel, c++ compilers, emace, etc.).  This is no different from how Dell laptops for sale at Office Depot come with the Windows OS and McAfee anti-virus program pre-installed.  \n",
      "    - AMIs come with many different types of (virtual) hardware. They will have different processor architectures, for example.  They can also have different flavors or quantites of RAM.  For our purposes, all the standard Linux architectures work equally well.\n",
      "        - In the car rental analogy, this is like specifying that you want a hybrid car with power steering, manual transmission, and antilock brakes, as opposed to a standard fuel, automatic transmission, V6-engine, pickup truck with a tow hitch and a sun roof.\n",
      "        - System archtecture is an important difference between Windows and Mac operating systems.  Rumor is that this has something to do with 'little endian' and 'big endian' encoding.  My ignorance about this hasn't yet caused *me* any problems, but keep this in mind if you find yourself trying to conjure up a hetergenous Windows + Linux cluster.\n",
      "        - System architecture is also important if you want to run a GPU cluster.  AWS has several GPU-based AMIs available.\n",
      "    - Many different AMIs are available.  \n",
      "         - Some are free; some aren't.  Even if an AMI is free, you still have to pay by the hour per machine running that AMI.  The charges depend on the instance size (see below), rather than the AMI type.\n",
      "         - The default AMI that Starcluster (more on that soon) uses is free, and it works fine for what we want to do.  You may later decide to find a different AMI from the list Amazon provides, but that's beyond the scope of this tutorial.\n",
      "             - You can also create your own customized AMIs.  That is also beyond the scope of this tutorial.\n",
      "\n",
      "\n",
      "+ **Instance Size**\n",
      "    - This specifies how big or small the AMI you picked out should be.\n",
      "        - 'Big' and 'small' refer to (among other things):\n",
      "            - How many processors each machine has\n",
      "            - How many units of RAM/other storage each machine has\n",
      "                - Example: many of the larger instance sizes have 8 cores per machine, along with several hundred GB of memory.\n",
      "        - Bigger instance sizes cost more.  \n",
      "        - The smallest instance size is the t1.micro, which costs \\$0.02/computing hour/instance. \n",
      "            - You get 750 t1.micro instance hours free (per year) when you create a new AWS account.  This lets you test out the system without worrying about making costly novice's mistakes.            \n",
      "        - Even the micro instances have decent computing power.  The largest instance size you should use while learning the system is the m1.small.  This costs ~$0.06/hour/instance, and is bigger and faster than my (admittedly old) local machine.\n",
      "        - The AWS website has more infomation on available instance sizes and pricing.\n",
      "    - In the car-rental analogy, this is like specifying whether you want an economy, mid-sized, or large car.  \n",
      "        - (This analogy is somewhat flawed, since most economy-sized cars have different architectures than mid-sized cars.  If pickups, vans, and convertables all came in economy, mid-sized, and large versions, the analogy would be more accurate.)\n",
      " \n",
      "+ **EBS**\n",
      "    - This stands for Elastic Block Storage\n",
      "    - It is an external storage medium for your virtual machines.  Think of it like any other type of external memory (flash drive, external hard drive), except it's in the cloud rather than in your pocket.\n",
      "        - We call individual EBS accounts or blocks 'volumes.'  This is because the neck-beards who built AWS are accustomed to discussing computer memory and file systems using terms like 'volumes' and 'NFS,' and they get to impose their jargon on the rest of us.\n",
      "        - The info on the EBS volume persists, even after you shut down your virtual machines.\n",
      "        - Just like a flash drive, you can attach an EBS volume to another virtual machine to have that computer 'see' the data it contains.\n",
      "    - You have to pay for it.\n",
      "    - You can have several distinct EBS volumes, just as you can own several different flash drives.\n",
      "    - You have to 'attach' an EBS volume to a given virtual machine in order for that computer to 'see' what's on it.  \n",
      "        - Starcluster handles most of this for you\n",
      "            - It probably does that using the 'mount' command, FYI.\n",
      "            \n",
      "+ **EC2**\n",
      "    - This stands for Elastic Cloud Computing \n",
      "    - This lets you automatically make your AWS cluster bigger or smaller, depending on how much data you need to crunch, and how much money you're willing to spend to temporarily add nodes to the cluster. \n",
      "        - The temporary additions to your cluster are called 'spot instances.'\n",
      "        - You specify how much you're willing to pay per spot instance.\n",
      "    - The details are only barely beyond the scope of this tutorial, and are worth reading up on. \n",
      "        - Starcluster has this capability built in.  Consult their documentation for more on how it works.\n",
      "\n",
      "## How is this related to Map-Reduce?\n",
      "If you've been trying to figure out how to set up an AWS cluster, you may have seen references to Map-Reduce.  This is just a way of describing how a group of computers work together on embarrassingly parallel problems.  If you're using a computer cluster, the cluster is using Map-Reduce, and that's why you're running into this term.\n",
      "\n",
      "Most articles on Map-Reduce were written for computer science experts, rather than scientists.  As such, the explanation can be difficult to decypher.  At its heart, though, Map-Reduce is a simple concept.\n",
      "\n",
      "Let's imagine a cluster of three computers.  You're using this cluster to scan a million different text files for the word 'moose.' \n",
      "\n",
      "Your inputs are the individual text files.  The process you're running on (i.e. 'applying to') each file is the 'see if this file contains the word \"moose\"' program. \n",
      "\n",
      "One computer (the master node) sends the process and inputs to each of the other two computers (the worker nodes).  When the workers are done, they hand off their results to the master node.  The master enters those results into a file on its local memory. Then, it tells each of the workers to apply that 'search for moose' program on a new file.  \n",
      "\n",
      "The 'map' of Map-Reduce refers to work that the master node does in handing out tasks and inputs to its workers, and in keeping track of which inputs still await processing.\n",
      "\n",
      "The 'reduce' of Map-Reduce refers to the second part of that process, where the workers hand their results back to the master node, which collates (or 'reduces') the results.\n",
      "\n",
      "*Personally, I envision Map-Reduce as 'five people chopping a pile of potatoes'.  One person is probably in charge of the whole thing, and tells everyone else whether to slice, dice, shave, or pulverize the potatoes.  That chief chef hands a new potato to each sous-chef as the sous-chef finishes chopping the previous potato.  The chief chef takes the chopped-up results from the sous-chefs and dumps them all together into a pot with all of the other potatoes that have been processed so far.  The chief chef keeps track of handing out and collecting the potatos; each worker processes one potato at a time, as it's passed off to them by the chief chef.*\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Starcluster: Your AWS Concierge\n",
      "The process of setting up a single machine, much less a cluster of interconnected nodes, on AWS is pretty involved.\n",
      "It requires you to keep track of lots of configuration files, login information, and Many Other Things That Aren't Your Research.\n",
      "\n",
      "Starcluster is an open-source program from MIT that manages all those details for you.  It takes care of logging you in to your AWS account, creating a cluster of machines (based on the AMIs and instance sizes you specify), enabling password-less ssh between nodes, attaching any EBS volumes you have to the cluster, setting up the master/worker node architecture, installing the queuing system, etc.  \n",
      "\n",
      "We will be using Starcluster to create our AWS clusters.\n",
      "\n",
      "I won't go into more detail on Starcluster here, because its own documentation does it better."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Install Starcluster on your local machine\n",
      "\n",
      "The Starcluster software is available [here](http://star.mit.edu/cluster/).  They provide documentation on how to download, install, and use the program.\n",
      "\n",
      "As part of the setup, you will need to create an AWS account. The Starcluster documentation will lead you through this.  Additional info on this part of the process can be found [here](http://aws.amazon.com/).\n",
      "\n",
      "After you install and configure Starcluster, consult their QuickStart guide to create and log in to your own cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Start up the cluster\n",
      "In this example, I'm running a 2 node t1.micro cluster.\n",
      "\n",
      "Start it using the usual Starcluster commands.\n",
      "\n",
      "It will take about 3-4 minutes to come online.\n",
      "\n",
      "**Below is part of the output from the startup process:**\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "...\n",
      ">>> Starting NFS server on master\n",
      ">>> Configuring NFS exports path(s):\n",
      "/home\n",
      ">>> Mounting all NFS export path(s) on 1 worker node(s)\n",
      "1/1 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \n",
      "...\n",
      ">>> Configuring NFS exports path(s):\n",
      "/opt/sge6\n",
      ">>> Mounting all NFS export path(s) on 1 worker node(s)\n",
      "...\n",
      ">>> Running plugin ipcluster\n",
      ">>> Writing IPython cluster config files\n",
      ">>> Starting the IPython controller and 1 engines on master\n",
      ">>> Waiting for JSON connector file... \n",
      "/Users/laurel/.starcluster/ipcluster/SecurityGroup:@sc-testcluster-us-east-1.json 100% || Time: 00:00:00   0.00 B/s\n",
      ">>> Authorizing tcp ports [1000-65535] on 0.0.0.0/0 for: IPython controller\n",
      ">>> Adding 1 engines on 1 nodes\n",
      "1/1 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \n",
      ">>> Setting up IPython web notebook for user: sgeadmin\n",
      ">>> Creating SSL certificate for user sgeadmin\n",
      ">>> Authorizing tcp ports [8888-8888] on 0.0.0.0/0 for: notebook\n",
      ">>> IPython notebook URL: https://ec2-54-226-244-156.compute-1.amazonaws.com:8888\n",
      ">>> The notebook password is: MIPYacE6b4OSLoji\n",
      "...\n",
      ">>> IPCluster has been started on SecurityGroup:@sc-testcluster for user 'sgeadmin'\n",
      "with 2 engines on 2 nodes.\n",
      "\n",
      "To connect to cluster from your local machine use:\n",
      "\n",
      "from IPython.parallel import Client\n",
      "client = Client('/Users/laurel/.starcluster/ipcluster/SecurityGroup:@sc-testcluster-us-east-1.json', sshkey='/Users/laurel/.ssh/mykey.rsa')\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Notes on errors:\n",
      "Once, I got inexplicable errors about 'cannot find any nodes' early in the Starcluster cluster launch process.\n",
      "\n",
      "To get around this, albeit kludge-ably, I changed the node(s) instance size in the ~/.starcluster/config file from m1.small to t1.micro.  The cluster then launched successfully.\n",
      "\n",
      "Not entirely sure why the m1.small instances suddenly stopped working for the AMI I'd been using.  Still using the same AMI for the t1.micro instances w/ no problems.\n",
      "\n",
      "It might also happen to you, so keep this fix in mind."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Info on how to set up and use a multiengine ipython cluster on an EC2 cluster\n",
      "\n",
      "##Ipython Notebook\n",
      "For what follows, you need to know:\n",
      "\n",
      "+ Python.  You don't need to know very much python, though.  As long as you know it's a programming language, and that you should Google the commands you need, you'll be fine.\n",
      "    - If you've never programmed before, Google 'introduction python tutorial' before you go further.  Figure out which of your class- or office-mates *have* written computer programs before, and go ask them for help whenever you get stuck or confused.\n",
      "    - If you've used Matlab, IDL, or Octave before, but have never written a script in any of those languages (i.e., you always just entered individual commands at the prompt), you'll need to learn how to do that before you go farther.  It will not take longer than two workdays to do this, at max.\n",
      "+ The iPython Notebook interface\n",
      "    - This is a Mathematica-esqe interactive python interface.  If you know python, it takes about five minutes to learn how to use the notebook interface.\n",
      "    - The rest of this tutorial uses the ipython notebook interface running on your AWS cluster to start, stop, and manage your parallel computations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Set up the ipython cluster controllers and engines on your EC2 cluster\n",
      "\n",
      "*Note:* My approach here is based on the one outlined by Ian Howson at http://www.ianhowson.com/how-to-set-up-a-private-ipython-cluster.html. \n",
      "\n",
      "The terminology for this whole endeavor can be confusing. \n",
      "\n",
      "The *ipython cluster* is a *software-based cluster* of multiple python engines/kernals working together through the python software.\n",
      "You can run an ipython cluster on a computer with a single processor; you'll just be running multiple (ipython) engines per (computer) CPU.\n",
      "\n",
      "The *AWS/EC2 cluster* is a group of (virtual) machines that are networked together to form a *\"hardware\" cluster*.  \n",
      "\n",
      "Startcluster sets up the hardware cluster for you.  It also installs the software and configuration files on each box that lets you run the ipython cluster program(s) (aka, ipcluster).\n",
      "\n",
      "----\n",
      "\n",
      "Once your EC2 cluster is up and running, you need to:\n",
      "\n",
      "1. Start an ipcontroller client on the master node as the user named <b>sgeadmin</b>.  \n",
      "    - Neck beard-y footnote: sgeadmin = Sun Grid Engine Administrator.  The Sun Grid Engine is one type of software that can control the flow of information between cluster nodes.  It's what the default Starcluster AMI uses.  Hadoop is another program that does about the same thing.  If you're concerned about what software your cluster conducts traffic control with, you can specify that in the Starcluster configuration file.  See the Starcluster documentation for more info on that.\n",
      "2. Start an ipengine on each of the nodes (master + workers)\n",
      "\n",
      "To do (1), log in to the master node like so:\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "starcluster sshmaster -u sgeadmin testcluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will log in to the master node as the user named sgeadmin.  \n",
      "\n",
      "At the master node's terminal prompt, enter:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "ipython profile create --parallel --profile=home"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, run the following command to edit the file:\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "emacs ~/.ipython/profile_home/ipcluster_config.py "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "to contain:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "c = get_config()\n",
      "\n",
      "c.IPClusterEngines.engine_launcher_class = 'SSH' \n",
      "c.LocalControllerLauncher.controller_args = [\"--ip='*'\"]\n",
      "\n",
      "c.SSHEngineSetLauncher.engines = {\n",
      "    'master': 1,\n",
      "    'node001': 1,\n",
      "}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Notes*:\n",
      "\n",
      "If you're running more than one worker node, add 'node002': 1, (and so on) to the last part of the code block above.\n",
      "\n",
      "If you want to run more than one engine on each node, change the '1' in that same code block to 'N', where N is the number of engines you want to run on that node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, the ipython cluster is configured and ready to start.\n",
      "\n",
      "To access it, go to the ipython notebook url that Starcluster provided you during its setup phase.  Enter the password it gave you.\n",
      "\n",
      "Go to the 'clusters' tab in the notebook dashboard.  \n",
      "\n",
      "Start the cluster called 'home'.  If you used a different name in the ipython cluster profile creation stage, use the cluster with that name, instead.\n",
      "\n",
      "Enter '2' in the '# of engines' field, and click 'start'. (If you have more than 2 nodes on your cluster, or want to run a different number of ipython engines, change the # of engines accordingly.)\n",
      "\n",
      "At this point, the ipython cluster is running and accessible by the ipython notebook running on the master node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Test the cluster you've just set up"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check that each node is running an ipython cluster engine before starting your heavy-duty parallel science code.\n",
      "\n",
      "Create an ipython notebook from the 'Notebooks' tab.  Enter the following commands there to test that all node-engines are up, running, and connected to the ipython cluster client/dispatch on the master node."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from IPython.parallel import Client "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "c = Client(profile='home')\n",
      "\n",
      "def checkhostname():\n",
      "    import socket\n",
      "    return socket.gethostname()\n",
      "\n",
      "balanced       = c.load_balanced_view()\n",
      "direct         = c[:]\n",
      "balanced.block = True\n",
      "direct.block   = True\n",
      "\n",
      "hostnames = direct.apply(checkhostname)\n",
      "hostnames\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If everything's working, the output should look like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "['master', 'node001']"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, if you have more nodes running engines, or if you have more than one engine running per node, the output should reflect that.  In the example I'm using here, the EC2 cluster has 2 nodes (one master, and one worker), and I'm only running one engine per node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "##Setting up fortran on the cluster/nodes\n",
      "**Note: My AWS_IPcluster_quickstart notebook contains another (arguably better) overview of this part of the process.**\n",
      "\n",
      "First, test that the /home directory on the master node is accessible to all the worker nodes.\n",
      "\n",
      "To do this, make a test.txt file and save it in the master node's home directory.\n",
      "\n",
      "Then, ssh into the worker node(s), and see if you can access test.txt.\n",
      "\n",
      "If you can, then run the following test in the notebook:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def catfile():\n",
      "    import os\n",
      "    contents = os.popen('cat test.txt').read()\n",
      "    return contents\n",
      "direct.apply(catfile)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output should look something like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "['This is ANOTHER test!\\n', 'This is ANOTHER test!\\n']"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check that your cluster has gfortran installed, type:\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "which gfortran"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "at the terminal prompt.  If it doesn't return anything, you don't have gfortran installed on that machine.  The AMI I'm using, however, comes with gfortran already installed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, write a little fortran program, and save it to the home directory on the master node.  Here is the fortran program I used, saved as hello.f.  The leading spaces are important!"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "      PRINT *,\"Hello World!\"\n",
      "      END\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compile the program on the master node, using gfortran:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "gfortran hello.f -o hello"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, run the following commands in the python notebook:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def fortrantest():\n",
      "    import os\n",
      "    contents = os.popen('./hello').read()\n",
      "    return contents\n",
      "direct.apply(fortrantest)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output should look like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "[' Hello World!\\n', ' Hello World!\\n']"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "##Installing and running the CodeForKozai fortran program on the cluster\n",
      "\n",
      "Initially, just test this out on the master node (to see if the micro instance I'm using has enough memory to handle a run).\n",
      "\n",
      "####(1) Upload all the necessary files (fortran & input) to the cluster"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "starcluster put testcluster ~/Desktop/Upload_to_AWS/ /home/fortran_stuff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####(2) Log into the master node"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "starcluster sshmaster mycluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####(3) Compile the fortran code in the master node's home directory"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "cd /home/sgeadmin/fortran_stuff\n",
      "gfortran full_tidal_heating_time_limited.f -o full_tidal_heating_time_limited"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Compiling the fortran code in the master node's home directory (rather than in the default, root, directory) lets you successfully run the code from other nodes, too.  *\n",
      "\n",
      "*I tested this by running Step 4 (below) in the home directory of node001, in addition to on the master node.*\n",
      "\n",
      "*It seemed to work fine.  I didn't run the node-simulation to completion, however.  I just let it run for a few seconds, to make sure that nothing crashed immediately.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####(4) Run the fortran code on the master node, capturing the output"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "./full_tidal_heating_time_limited < inputs/test_for_time_limited_tidal_calcs.txt > outputs/test_for_time_limited_tidal_calcs_output.txt &\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Baby steps towards getting a fortran program to run in an async/load-balanced way on the micro-cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a toy function, 'thinger'\n",
      "def thinger(input):\n",
      "    import socket\n",
      "    import os\n",
      "    contents = os.popen('./hello '+str(input)).read()\n",
      "    hostname =  socket.gethostname()\n",
      "    output = hostname + ' returns '+contents\n",
      "    return output\n",
      "\n",
      "# Test it out on the load-balanced view\n",
      "test = balanced.map(thinger,range(50))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After getting 'thinger' to work, I wrote the function below and tested it on real, though brief, simulations. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_sims(input_filenum):\n",
      "    import socket, os\n",
      "    foo = os.system('./full_tidal_heating_time_limited < inputs/input'+str(input_filenum)+'.txt > outputs/output'+str(input_filenum)+'.txt')\n",
      "    print foo\n",
      "    hostname =  socket.gethostname()\n",
      "    hostname = hostname + ' ran '+ str(input_filenum)\n",
      "    return hostname + '  '+str(foo)\n",
      "\n",
      "# Test it out on the load-balanced view\n",
      "test = balanced.map(run_sims,[1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, using run_sims in parallel (using both direct and load balanced views) exposed the following error:\n",
      "\n",
      "<blockquote>\"Right now, when I run 'run_sims(1)' or similar on its *own*, it writes outputs to output/output1.[txt/mod] just fine.\n",
      "\n",
      "However, when I run any kind of [direct/balanaced] map or apply, the processes do **not** write any outputs to the outputs directory.\n",
      "Maybe it's a permissions issue?  Since I'm running ipython on the micro-server as sgeadmin (rather than root).\" </blockquote>\n",
      "\n",
      "\n",
      "This morning, I've been trying to sort this out.  Google has yielded the <a href=\"http://www.parallelpython.com/component/option,com_smf/Itemid,1/topic,103.0\">following</a>:\n",
      "\n",
      "<blockquote > os.system command overrides streams redirected by pp and therefor pp crashes.\n",
      "The solution is to use:<br><br>\n",
      "print os.popen(\"yourcommand\").read()\n",
      "<br> \n",
      "<br>\n",
      "instead of <br><br>\n",
      "os.system(\"yourcommand\")\n",
      "</blockquote>\n",
      "\n",
      "Updating the run_sims routine to see if that changes fixes my problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_sims(input_filenum):\n",
      "    import socket, os\n",
      "    foo = os.popen('./full_tidal_heating_time_limited < inputs/input'+str(input_filenum)+'.txt > outputs/output'+str(input_filenum)+'.txt')\n",
      "    print foo\n",
      "    hostname =  socket.gethostname()\n",
      "    hostname = hostname + ' ran '+ str(input_filenum)\n",
      "    return hostname + '  '+str(foo)\n",
      "\n",
      "# Test it out on the load-balanced view\n",
      "test = balanced.map(run_sims,[1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Result: problem still not fixed.   No output files were written.  Unclear if the fortran program even ran.<br>\n",
      "Process returns the following output:<br>\n",
      "<blockquote>\n",
      "[\"node001 ran 1  <open file './full_tidal_heating_time_limited < inputs/input1.txt > outputs/output1.txt', mode 'r' at 0x94b3b20>\",\n",
      " \"master ran 2  <open file './full_tidal_heating_time_limited < inputs/input2.txt > outputs/output2.txt', mode 'r' at 0x9a93b20>\",\n",
      " \"node001 ran 3  <open file './full_tidal_heating_time_limited < inputs/input3.txt > outputs/output3.txt', mode 'r' at 0x94b3b20>\"]\n",
      "\n",
      "</blockquote>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trying out a simple file redirect within python also fails, with this error:<br>\n",
      "\n",
      "<blockquote>\n",
      "<p>In [34]: os.popen('ls>foo.txt')\n",
      "\n",
      "<p>Out[34]: <open file 'ls>foo.txt', mode 'r' at 0xa6b4d30>\n",
      "\n",
      "<p>In [35]: sh: 1: cannot create foo.txt: Permission denied\n",
      "</blockquote>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perhaps it's a user/permissions issue?  Trying this again, logged in as root instead of as sgeadmin.\n",
      "\n",
      "*The python cluster setup wouldn't work when I logged in as root. Some stupidity about .json files, not finding them, etc.*"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg(input_filenum):\n",
      "#    import os, pickle\n",
      "    f = open('/tmp/foo'+str(input_filenum)+'.txt', 'w')\n",
      "    f.write(str(input_filenum))\n",
      "    f.close()\n",
      "    return input_filenum\n",
      "\n",
      "# Test it out on its own\n",
      "arg(1)\n",
      "\n",
      "# Test it out on the load-balanced view\n",
      "test = balanced.map(arg,[1,2,3])\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg2(input_filenum):\n",
      "    import os\n",
      "    raw_output = os.popen('./full_tidal_heating_time_limited < inputs/input'+str(input_filenum)+'.txt' ).read()\n",
      "    print raw_output\n",
      "    return raw_output\n",
      "\n",
      "# Test it out on its own\n",
      "arg2(1)\n",
      "\n",
      "# Test it out on the load-balanced view\n",
      "test = balanced.map(arg2,[1,2,3])"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg3(input_filenum):\n",
      "    import os\n",
      "    raw_output = os.popen('./full_tidal_heating_time_limited < inputs/input'+str(input_filenum)+'.txt' ).read()\n",
      "    f = open('/tmp/run'+str(input_filenum)+'.txt', 'w')\n",
      "    f.write(raw_output)\n",
      "    f.close()\n",
      "    return 0\n",
      "\n",
      "foo = arg3(1)\n",
      "print foo\n",
      "\n",
      "test = balanced.map(arg3,[1,2,3])\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg4(input_filename):\n",
      "    import os, subprocess as sub\n",
      "    cmd = './full_tidal_heating_time_limited < ' + input_filename+'.txt'\n",
      "    temp = sub.Popen(cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "    output, errors = temp.communicate()\n",
      "#    f = open('/tmp/run'+str(input_filenum)+'.txt', 'w')\n",
      "#    f.write(raw_output)\n",
      "#    f.close()\n",
      "    return errors\n",
      "\n",
      "foo = arg4('inputs/input1')\n",
      "print foo\n",
      "\n",
      "test = balanced.map(arg4,['inputs/input1','inputs/input2'])\n",
      "print test\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " From the block of code above, we get the following clues/errors:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In [210]: foo = arg4('inputs/input1')\n",
      "\n",
      "In [211]: print foo\n",
      "\n",
      "\n",
      "In [212]: \n",
      "\n",
      "In [212]: test = balanced.map(arg4,['inputs/input1','inputs/input2'])\n",
      "\n",
      "In [213]: print test\n",
      "['/bin/sh: 1: cannot open inputs/input1.txt: No such file\\n', '/bin/sh: 1: cannot open inputs/input2.txt: No such file\\n']\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why are the input files visible to arg4 when it runs on its own, but not when it's running the the parallel framework?\n",
      "\n",
      "Would specifying the entire file path to the inputs solve this problem?"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg5(input_filename):\n",
      "    import os, subprocess as sub\n",
      "    cmd = './full_tidal_heating_time_limited < /home/sgeadmin/' + input_filename+'.txt'\n",
      "    temp = sub.Popen(cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "    output, errors = temp.communicate()\n",
      "    return output, errors\n",
      "\n",
      "foo = arg5('input1')\n",
      "print foo[1],len(foo[0])\n",
      "\n",
      "\n",
      "test = balanced.map(arg5,['input1','input2'])\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def arg6(input_filename):\n",
      "    import os, subprocess as sub\n",
      "#    cmd = './hello ' + input_filename\n",
      "    cmd = 'pwd'\n",
      "    temp = sub.Popen(cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "    output, errors = temp.communicate()\n",
      "    return output, errors\n",
      "\n",
      "foo = arg6('1')\n",
      "print foo[1],len(foo[0])\n",
      "\n",
      "\n",
      "test = balanced.map(arg6,['1','2'])\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the tests above (arg5), I discovered that /home/sgeadmin is the shared directory on the cluster (when logged in as user = sgeadmin).  \n",
      "\n",
      "Still can't get the python cluster running (some error message about a security .json file not being there) when logged into the cluster as root.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----\n",
      "## Processing/parsing the simulation outputs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arg7(input_filename):\n",
      "    import os, subprocess as sub\n",
      "    cmd = './full_tidal_heating_time_limited < /home/sgeadmin/' + input_filename+'.txt'\n",
      "    temp = sub.Popen(cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "    output, errors = temp.communicate()\n",
      "    return output, errors\n",
      "\n",
      "\n",
      "singleTest = arg7('input1')\n",
      "batchTest = balanced.map(arg7,['input1','input2','input3'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arg8(input_filename):\n",
      "    import os, subprocess as sub\n",
      "    from call_me_a_newt import call_me_a_newt\n",
      "    name = call_me_a_newt(input_filename)\n",
      "    cmd = 'ls'\n",
      "    temp = sub.Popen(cmd,stdout=sub.PIPE,stderr = sub.PIPE, shell = True)\n",
      "    output,errors = temp.communicate()\n",
      "    return len(output), name\n",
      "\n",
      "\n",
      "batchTest = balanced.map(arg8,['YOUR','MOM','ROCKS'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arg9(input_filename):\n",
      "    import os, subprocess as sub\n",
      "    from my_python_tools import parse_kozai_results_on_cluster\n",
      "    #\n",
      "    run_cmd = './full_tidal_heating_time_limited < /home/sgeadmin/inputs/' + input_filename+'.txt'\n",
      "    run_temp = sub.Popen(run_cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "    run_output, run_errors = run_temp.communicate()\n",
      "#\n",
      "    test = parse_kozai_results_on_cluster(input_filename,run_output)\n",
      "    print test\n",
      "#    zip_cmd = 'tar -cvzf '+test+'full_run_output.tar.gz '+test+'full_run_output.txt' \n",
      "#    print zip_cmd\n",
      "#    zip_temp = sub.Popen(zip_cmd,stdout=sub.PIPE,stderr=sub.PIPE, shell=True)\n",
      "#    zip_output,zip_errors = zip_temp.communicate()\n",
      "#\n",
      "#    return run_errors#,zip_errors\n",
      "    return test\n",
      "\n",
      "singleTest = arg9('run1')\n",
      "\n",
      "batchTest = balanced.map(arg9,['run1','run2','run3'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      ">>> IPython notebook URL: https://ec2-23-22-179-47.compute-1.amazonaws.com:8888\n",
      ">>> The notebook password is: Iy0DJuYmxS42wP6q\n",
      "*** WARNING - Please check your local firewall settings if you're having\n",
      "*** WARNING - issues connecting to the IPython notebook\n",
      ">>> IPCluster has been started on SecurityGroup:@sc-testcluster for user 'sgeadmin'\n",
      "with 2 engines on 2 nodes.\n",
      "\n",
      "To connect to cluster from your local machine use:\n",
      "\n",
      "from IPython.parallel import Client\n",
      "client = Client('/Users/laurel/.starcluster/ipcluster/SecurityGroup:@sc-testcluster-us-east-1.json', sshkey='/Users/laurel/.ssh/mykey.rsa')\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}